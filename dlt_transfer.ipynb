{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41f45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tp_logger.config import LoggerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8809d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41ec035",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dlt.pipeline(\n",
    "  pipeline_name='test_logg_data_pipeline',\n",
    "  destination=dlt.destinations.duckdb(\"shared.duckdb\"),\n",
    "  dataset_name='test_logg_data',\n",
    "  dev_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5892aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables and schemas:\n",
      "┌──────────┬────────────────┬─────────────────────┬───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
      "│ database │     schema     │        name         │                                                                                                       column_names                                                                                                        │                                                                          column_types                                                                          │ temporary │\n",
      "│ varchar  │    varchar     │       varchar       │                                                                                                         varchar[]                                                                                                         │                                                                           varchar[]                                                                            │  boolean  │\n",
      "├──────────┼────────────────┼─────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
      "│ shared   │ logs           │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                                                                                                                                          │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                                                                                  │ false     │\n",
      "│ shared   │ logs           │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id]                                                                                                                          │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                                                                        │ false     │\n",
      "│ shared   │ logs           │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                                                                                                                                                 │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                                                                                          │ false     │\n",
      "│ shared   │ logs           │ job_logs            │ [id, project_name, module_name, run_id, timestamp, level, message, context__stage, _dlt_load_id, _dlt_id, context__records, context__status, context__usage_pct, context__service, context__timeout_ms, context__success] │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, BIGINT, VARCHAR, BIGINT, VARCHAR, BIGINT, BOOLEAN] │ false     │\n",
      "│ shared   │ simple_test_db │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                                                                                                                                          │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                                                                                  │ false     │\n",
      "│ shared   │ simple_test_db │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id]                                                                                                                          │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                                                                        │ false     │\n",
      "│ shared   │ simple_test_db │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                                                                                                                                                 │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                                                                                          │ false     │\n",
      "│ shared   │ simple_test_db │ job_logs            │ [id, project_name, module_name, run_id, timestamp, level, message, _dlt_load_id, _dlt_id]                                                                                                                                 │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]                                                             │ false     │\n",
      "└──────────┴────────────────┴─────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴───────────┘\n",
      "\n",
      "\n",
      "Job logs table schema:\n",
      "job_logs table not found - run main.py first to create logs\n",
      "\n",
      "Sample log entries:\n",
      "No log data found - run main.py first to create logs\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import dlt\n",
    "from tp_logger.config import get_config\n",
    "\n",
    "# Get the tp-logger configuration to find the correct DB path\n",
    "config = get_config()\n",
    "\n",
    "# Connect to the same DuckDB file that tp-logger is using\n",
    "db = duckdb.connect(\"shared.duckdb\")\n",
    "\n",
    "# Create a pipeline that connects to the existing DuckDB file\n",
    "p = dlt.pipeline(\n",
    "    pipeline_name=config.pipeline_name,  # Use the same pipeline name\n",
    "    destination=dlt.destinations.duckdb(db),  # Pass the connected db instance\n",
    "    dataset_name=config.dataset_name,  # Use the same dataset name\n",
    "    dev_mode=False\n",
    ")\n",
    "\n",
    "# Now you can query the database to see what tables exist\n",
    "print(\"Available tables and schemas:\")\n",
    "print(db.sql(\"DESCRIBE;\"))\n",
    "\n",
    "# You can also query specific tables created by tp-logger\n",
    "print(\"\\nJob logs table schema:\")\n",
    "try:\n",
    "    print(db.sql(\"DESCRIBE tp_logger_logs.job_logs;\"))\n",
    "except:\n",
    "    print(\"job_logs table not found - run main.py first to create logs\")\n",
    "\n",
    "# Query the actual log data\n",
    "print(\"\\nSample log entries:\")\n",
    "try:\n",
    "    print(db.sql(\"SELECT * FROM tp_logger_logs.job_logs LIMIT 5;\"))\n",
    "except:\n",
    "    print(\"No log data found - run main.py first to create logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6c4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to target DuckDB database for transfer.\n",
      "Created transfer pipeline for logs_data.\n",
      "=== Checking source data ===\n",
      "Schemas in source database:\n",
      "┌────────────────────┬────────────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
      "│      database      │     schema     │        name         │                                                                                 column_names                                                                                 │                                                                         column_types                                                                         │ temporary │\n",
      "│      varchar       │    varchar     │       varchar       │                                                                                  varchar[]                                                                                   │                                                                          varchar[]                                                                           │  boolean  │\n",
      "├────────────────────┼────────────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
      "│ tp_logger_pipeline │ tp_logger_logs │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                                                                                             │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                                                                                │ false     │\n",
      "│ tp_logger_pipeline │ tp_logger_logs │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id]                                                                             │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                                                                      │ false     │\n",
      "│ tp_logger_pipeline │ tp_logger_logs │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                                                                                                    │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                                                                                        │ false     │\n",
      "│ tp_logger_pipeline │ tp_logger_logs │ job_logs            │ [id, project_name, module_name, function_name, run_id, timestamp, level, action, message, success, status_code, duration_ms, request_method, context, _dlt_load_id, _dlt_id] │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, BOOLEAN, BIGINT, BIGINT, VARCHAR, JSON, VARCHAR, VARCHAR] │ false     │\n",
      "└────────────────────┴────────────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴───────────┘\n",
      "\n",
      "\n",
      "Sample data from logs.job_logs:\n",
      "┌──────────────┐\n",
      "│ record_count │\n",
      "│    int64     │\n",
      "├──────────────┤\n",
      "│           22 │\n",
      "└──────────────┘\n",
      "\n",
      "Attempting to read from logs.job_logs...\n",
      "[('9bd7c618-0ade-49d7-811a-3ec78345227f', 'example_app', '__main__', None, '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 41, 221615, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', None, 'Application finished', None, None, None, None, '{}', '1754308661.2305799', 'K6SiWfTABW7eOw'), ('271b12fa-2668-4132-8bef-b22189cf9b82', 'example_app', '__main__', None, '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 41, 157583, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'ERROR', 'risky_operation', 'Exception in risky_operation: Something went wrong!', False, None, None, None, '{\"exception_type\":\"ValueError\"}', '1754308661.166517', 'DFrXmzLnE96BOA'), ('450c07bf-3918-4cd9-ad2b-1361f40d2185', 'example_app', '__main__', 'risky_operation', '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 41, 93026, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'ERROR', 'risky_operation_execution', 'Failed risky_operation_execution after 62ms', False, None, 62, None, '{}', '1754308661.101797', 'q4ZX1uszHjVrjQ'), ('57e6d3f1-7b59-49a2-b754-143b77a16bd5', 'example_app', '__main__', None, '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 41, 29651, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'ERROR', 'risky_operation_execution', 'Exception in risky_operation_execution: Something went wrong!', False, None, None, None, '{\"exception_type\":\"ValueError\"}', '1754308661.038049', '2cpZPqKGU4P3Rg'), ('061bd415-c158-46e5-a395-02cba33935ca', 'example_app', '__main__', 'risky_operation', '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 40, 967777, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', 'risky_operation_execution', 'Starting risky_operation_execution', True, None, None, None, '{}', '1754308660.9767182', 'h760RZcSVJR6tg'), ('9c1a0d79-5695-4cb4-a4f1-82ab47dcda7e', 'example_app', '__main__', 'process_data', '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 40, 882726, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', 'data_processing', 'Completed data_processing in 167ms', True, None, 167, None, '{}', '1754308660.908831', '8x4yf84YhpFcfg'), ('7b885803-9fb3-4540-9b0b-6b5bbbbb02ff', 'example_app', '__main__', 'process_data', '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 40, 715282, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', 'data_processing', 'Starting data_processing', True, None, None, None, '{}', '1754308660.723802', '/vElQCIrC9+4GA'), ('aa4f3b9f-cad2-45ae-aabf-a890cbbc0b7f', 'example_app', '__main__', None, '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 40, 650363, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', 'initialization', 'App initialized successfully', True, None, 150, None, '{\"version\":\"1.0.0\"}', '1754308660.660477', 't+nl8g2xMBQZ8w'), ('54084b07-0ab1-493b-96c3-8fb2731f31b7', 'example_app', '__main__', None, '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 40, 587031, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'WARNING', None, 'This is a warning', None, None, None, None, '{}', '1754308660.595284', 'nv9zzw8m+hnyFg'), ('405a03ac-9bf4-4018-b0bc-6dcc8089bf18', 'example_app', '__main__', None, '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 40, 508502, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'DEBUG', None, 'This is a debug message', None, None, None, None, '{}', '1754308660.5260942', 'CGBAym/fWtHmNA'), ('554dad77-4962-4513-b6c5-e31954967f08', 'example_app', '__main__', None, '56a1af4b-c31c-4427-accc-472a2f39f82e', datetime.datetime(2025, 8, 4, 15, 57, 40, 320999, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', None, 'Application started', None, None, None, None, '{}', '1754308660.44477', 'iRpgri2YC2B2qg'), ('0a6e92f3-8ec5-47ea-a42f-32144270676a', 'example_app', '__main__', None, 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 976902, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', None, 'Application finished', None, None, None, None, '{}', '1754305475.984627', 'q7Nzm/fNTxaF0A'), ('c1a24e9c-11ea-475e-8333-0248a1e26f3f', 'example_app', '__main__', None, 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 914981, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'ERROR', 'risky_operation', 'Exception in risky_operation: Something went wrong!', False, None, None, None, '{\"exception_type\":\"ValueError\"}', '1754305475.922604', 'imuQPpQU295ugg'), ('ea08044f-5f80-4d1d-bf2e-94ca60f30dec', 'example_app', '__main__', 'risky_operation', 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 851962, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'ERROR', 'risky_operation_execution', 'Failed risky_operation_execution after 64ms', False, None, 64, None, '{}', '1754305475.859924', 'Tu9GKAgEasoGnA'), ('65ee8b3a-1bd2-4b96-9f7c-5cb0cc4d9f08', 'example_app', '__main__', None, 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 779041, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'ERROR', 'risky_operation_execution', 'Exception in risky_operation_execution: Something went wrong!', False, None, None, None, '{\"exception_type\":\"ValueError\"}', '1754305475.787412', '671A9wO4ZPy6zg'), ('81306d87-20f8-4463-9a04-bb2ed794cc64', 'example_app', '__main__', 'risky_operation', 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 714271, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', 'risky_operation_execution', 'Starting risky_operation_execution', True, None, None, None, '{}', '1754305475.7222369', 'Z0l1qxGUDPcQ6g'), ('5bcef944-38fd-4233-9173-caf60c84fa9a', 'example_app', '__main__', 'process_data', 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 626334, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', 'data_processing', 'Completed data_processing in 162ms', True, None, 162, None, '{}', '1754305475.635776', 'EWbCZD8Zd0Q2Lw'), ('d61f4401-50f4-4703-a055-ee31f6b46fb9', 'example_app', '__main__', 'process_data', 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 463395, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', 'data_processing', 'Starting data_processing', True, None, None, None, '{}', '1754305475.4709692', '1/T6TxXe46pJ2A'), ('b79b5a6f-c0db-4fbd-8a79-16a5e6b0d8da', 'example_app', '__main__', None, 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 400708, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', 'initialization', 'App initialized successfully', True, None, 150, None, '{\"version\":\"1.0.0\"}', '1754305475.40893', 'EdpVEw/4haYFTw'), ('95c9abce-a649-474f-bbb0-3464f86b750c', 'example_app', '__main__', None, 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 338736, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'WARNING', None, 'This is a warning', None, None, None, None, '{}', '1754305475.346411', 'k5oLuLT5gBvAqA'), ('87099410-886f-44b5-9c32-201c8c0391d8', 'example_app', '__main__', None, 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 272685, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'DEBUG', None, 'This is a debug message', None, None, None, None, '{}', '1754305475.280603', 'OsrtgkM4sqsGLA'), ('7f463f7d-5235-4e98-8ee9-af480de488e6', 'example_app', '__main__', None, 'daf1d1e4-9a38-47d5-8020-0d582e76108a', datetime.datetime(2025, 8, 4, 15, 4, 35, 64827, tzinfo=<DstTzInfo 'Europe/Madrid' CEST+2:00:00 DST>), 'INFO', None, 'Application started', None, None, None, None, '{}', '1754305475.175741', 'wfG0WVbDq6VHVg')]\n",
      "['id', 'project_name', 'module_name', 'function_name', 'run_id', 'timestamp', 'level', 'action', 'message', 'success', 'status_code', 'duration_ms', 'request_method', 'context', '_dlt_load_id', '_dlt_id']\n",
      "Found 22 records in logs.job_logs\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (2362381431.py, line 73)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31myield dict(zip(columns, row))\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import dlt\n",
    "from typing import Iterator, Dict, Any\n",
    "\n",
    "# Connect to the source database (keep this connection open)\n",
    "source_db = duckdb.connect(\"tp_logger_pipeline.duckdb\")\n",
    "\n",
    "def job_logs_resource() -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A DLT resource that reads job logs from the source DuckDB database.\n",
    "    \"\"\"\n",
    "    # Create a fresh connection inside the resource function\n",
    "    temp_db = duckdb.connect(\"tp_logger_pipeline.duckdb\")\n",
    "    \n",
    "    # Query the job_logs table from the logs schema (which has more columns)\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        id\n",
    "    FROM tp_logger_logs.job_logs\n",
    "    ORDER BY timestamp DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"Attempting to read from logs.job_logs...\")\n",
    "        result = temp_db.execute(query).fetchall()\n",
    "        print(result)\n",
    "        columns = [desc[0] for desc in temp_db.description]\n",
    "        print(columns)\n",
    "        \n",
    "        print(f\"Found {len(result)} records in logs.job_logs\")\n",
    "        for row in result:\n",
    "            yield dict(zip(columns, row))\n",
    "\n",
    "    finally:\n",
    "        temp_db.close()\n",
    "\n",
    "# Create a new DuckDB database for the transferred data\n",
    "target_db = duckdb.connect(\"transferred_logs.duckdb\")\n",
    "print(\"Connected to target DuckDB database for transfer.\")\n",
    "# Create a new DLT pipeline for the transfer with different dataset name\n",
    "transfer_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"log_transfer_pipeline\",\n",
    "    destination=dlt.destinations.duckdb(target_db),\n",
    "    dataset_name=\"logs_data\",\n",
    "    dev_mode=False\n",
    ")\n",
    "print(\"Created transfer pipeline for logs_data.\")\n",
    "# First, let's check what data is available in the source\n",
    "print(\"=== Checking source data ===\")\n",
    "try:\n",
    "    print(\"Schemas in source database:\")\n",
    "    print(source_db.sql(\"DESCRIBE;\"))\n",
    "    \n",
    "    print(\"\\nSample data from logs.job_logs:\")\n",
    "    print(source_db.sql(\"SELECT COUNT(*) as record_count FROM tp_logger_logs.job_logs;\"))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error checking source data: {e}\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM tp_logger_logs.job_logs\n",
    "    ORDER BY timestamp DESC\n",
    "    \"\"\"\n",
    "print(\"Attempting to read from logs.job_logs...\")\n",
    "result = source_db.execute(query).fetchall()\n",
    "print(result)\n",
    "columns = [desc[0] for desc in source_db.description]\n",
    "print(columns)\n",
    "print(f\"Found {len(result)} records in logs.job_logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7cb8513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 11:20:35,662|[WARNING]|8536|8458690304|dlt|pipeline.py|_set_destinations:1441|The destination athena requires the filesystem staging destination to be set, but it was not provided. Setting it to 'filesystem'.\n",
      "2025-08-05 11:20:37,263|[WARNING]|8536|8458690304|dlt|pipeline.py|run:722|The pipeline `run` method will now load the pending load packages. The data you passed to the run function will not be loaded. In order to do that you must run the pipeline again\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline log_transfer load step completed in 8.54 seconds\n",
      "1 load package(s) were loaded to destination athena and into dataset transferred_logs\n",
      "The filesystem staging destination used s3://prod-ddbb-data/temp/ location to stage data\n",
      "The athena destination used s3://prod-ddbb-data/temp/ on awsdatacatalog location to store data\n",
      "Load package 1754385330.507402 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import dlt\n",
    "import duckdb\n",
    "from typing import Iterator, Dict, Any\n",
    "\n",
    "@dlt.resource(name=\"job_logs\", write_disposition=\"replace\")\n",
    "def job_logs_resource() -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A DLT resource that reads all job logs from the source DuckDB database.\n",
    "    \"\"\"\n",
    "    # A resource should be self-contained and create its own connection\n",
    "    with duckdb.connect(\"tp_logger_pipeline.duckdb\", read_only=True) as conn:\n",
    "        \n",
    "        # Query all columns from the job_logs table\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM tp_logger_logs.job_logs\n",
    "        ORDER BY timestamp DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = conn.execute(query)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Yield each row as a dictionary\n",
    "        # This streams results without loading the whole table into memory\n",
    "        for row in cursor.fetchall():\n",
    "            yield dict(zip(columns, row))\n",
    "\n",
    "# You can now use this resource in a dlt pipeline, for example:\n",
    "\n",
    "# 1. Create a pipeline to a new destination, disabling Lake Formation\n",
    "transfer_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"log_transfer\",\n",
    "    destination=dlt.destinations.athena(lakeformation_config=None),\n",
    "    dataset_name=\"transferred_logs\"\n",
    ")\n",
    "\n",
    "source_db.close()\n",
    "\n",
    "# 2. Run the pipeline with the resource\n",
    "load_info = transfer_pipeline.run(job_logs_resource())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d4328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fbafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdb4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f61f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp-logger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
