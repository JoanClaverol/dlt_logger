{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f45bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8809d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41ec035",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dlt.pipeline(\n",
    "  pipeline_name='test_logg_data_pipeline',\n",
    "  destination=dlt.destinations.duckdb(\"shared.duckdb\"),\n",
    "  dataset_name='test_logg_data',\n",
    "  dev_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import duckdb\n",
    "\n",
    "from dlt_logger.config import get_config\n",
    "\n",
    "# Get the dlt-logger configuration to find the correct DB path\n",
    "config = get_config()\n",
    "\n",
    "# Connect to the same DuckDB file that dlt-logger is using\n",
    "db = duckdb.connect(\"shared.duckdb\")\n",
    "\n",
    "# Create a pipeline that connects to the existing DuckDB file\n",
    "p = dlt.pipeline(\n",
    "    pipeline_name=config.pipeline_name,  # Use the same pipeline name\n",
    "    destination=dlt.destinations.duckdb(db),  # Pass the connected db instance\n",
    "    dataset_name=config.dataset_name,  # Use the same dataset name\n",
    "    dev_mode=False\n",
    ")\n",
    "\n",
    "# Now you can query the database to see what tables exist\n",
    "print(\"Available tables and schemas:\")\n",
    "print(db.sql(\"DESCRIBE;\"))\n",
    "\n",
    "# You can also query specific tables created by dlt-logger\n",
    "print(\"\\nJob logs table schema:\")\n",
    "try:\n",
    "    print(db.sql(\"DESCRIBE dlt_logger_logs.job_logs;\"))\n",
    "except:\n",
    "    print(\"job_logs table not found - run main.py first to create logs\")\n",
    "\n",
    "# Query the actual log data\n",
    "print(\"\\nSample log entries:\")\n",
    "try:\n",
    "    print(db.sql(\"SELECT * FROM dlt_logger_logs.job_logs LIMIT 5;\"))\n",
    "except:\n",
    "    print(\"No log data found - run main.py first to create logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterator\n",
    "from typing import Any\n",
    "\n",
    "import dlt\n",
    "import duckdb\n",
    "\n",
    "# Connect to the source database (keep this connection open)\n",
    "source_db = duckdb.connect(\"dlt_logger_pipeline.duckdb\")\n",
    "\n",
    "def job_logs_resource() -> Iterator[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A DLT resource that reads job logs from the source DuckDB database.\n",
    "    \"\"\"\n",
    "    # Create a fresh connection inside the resource function\n",
    "    temp_db = duckdb.connect(\"dlt_logger_pipeline.duckdb\")\n",
    "\n",
    "    # Query the job_logs table from the logs schema (which has more columns)\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        id\n",
    "    FROM dlt_logger_logs.job_logs\n",
    "    ORDER BY timestamp DESC\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"Attempting to read from logs.job_logs...\")\n",
    "        result = temp_db.execute(query).fetchall()\n",
    "        print(result)\n",
    "        columns = [desc[0] for desc in temp_db.description]\n",
    "        print(columns)\n",
    "\n",
    "        print(f\"Found {len(result)} records in logs.job_logs\")\n",
    "        for row in result:\n",
    "            yield dict(zip(columns, row))\n",
    "\n",
    "    finally:\n",
    "        temp_db.close()\n",
    "\n",
    "# Create a new DuckDB database for the transferred data\n",
    "target_db = duckdb.connect(\"transferred_logs.duckdb\")\n",
    "print(\"Connected to target DuckDB database for transfer.\")\n",
    "# Create a new DLT pipeline for the transfer with different dataset name\n",
    "transfer_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"log_transfer_pipeline\",\n",
    "    destination=dlt.destinations.duckdb(target_db),\n",
    "    dataset_name=\"logs_data\",\n",
    "    dev_mode=False\n",
    ")\n",
    "print(\"Created transfer pipeline for logs_data.\")\n",
    "# First, let's check what data is available in the source\n",
    "print(\"=== Checking source data ===\")\n",
    "try:\n",
    "    print(\"Schemas in source database:\")\n",
    "    print(source_db.sql(\"DESCRIBE;\"))\n",
    "\n",
    "    print(\"\\nSample data from logs.job_logs:\")\n",
    "    print(source_db.sql(\"SELECT COUNT(*) as record_count FROM dlt_logger_logs.job_logs;\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking source data: {e}\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM dlt_logger_logs.job_logs\n",
    "    ORDER BY timestamp DESC\n",
    "    \"\"\"\n",
    "print(\"Attempting to read from logs.job_logs...\")\n",
    "result = source_db.execute(query).fetchall()\n",
    "print(result)\n",
    "columns = [desc[0] for desc in source_db.description]\n",
    "print(columns)\n",
    "print(f\"Found {len(result)} records in logs.job_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterator\n",
    "from typing import Any\n",
    "\n",
    "import dlt\n",
    "import duckdb\n",
    "\n",
    "\n",
    "@dlt.resource(name=\"job_logs\", write_disposition=\"replace\")\n",
    "def job_logs_resource() -> Iterator[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A DLT resource that reads all job logs from the source DuckDB database.\n",
    "    \"\"\"\n",
    "    # A resource should be self-contained and create its own connection\n",
    "    with duckdb.connect(\"dlt_logger_pipeline.duckdb\", read_only=True) as conn:\n",
    "\n",
    "        # Query all columns from the job_logs table\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            *\n",
    "        FROM dlt_logger_logs.job_logs\n",
    "        ORDER BY timestamp DESC\n",
    "        \"\"\"\n",
    "\n",
    "        cursor = conn.execute(query)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Yield each row as a dictionary\n",
    "        # This streams results without loading the whole table into memory\n",
    "        for row in cursor.fetchall():\n",
    "            yield dict(zip(columns, row))\n",
    "\n",
    "# You can now use this resource in a dlt pipeline, for example:\n",
    "\n",
    "# 1. Create a pipeline to a new destination, disabling Lake Formation\n",
    "transfer_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"log_transfer\",\n",
    "    destination=dlt.destinations.athena(lakeformation_config=None),\n",
    "    dataset_name=\"transferred_logs\"\n",
    ")\n",
    "\n",
    "source_db.close()\n",
    "\n",
    "# 2. Run the pipeline with the resource\n",
    "load_info = transfer_pipeline.run(job_logs_resource())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d4328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fbafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdb4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f61f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp-logger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}